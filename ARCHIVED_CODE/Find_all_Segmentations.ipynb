{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_directory=r\"D:\\PanCanAID-Batch1-202408\"\n",
    "patient_directory=r\"D:\\PanCanAID-Batch1-202408\\2072\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%pip install pydicom pandas matplotlib numpy\n",
    "#%pip install GDCM pylibjpeg pylibjpeg-libjpeg pylibjpeg-openjpeg  # required packages for showing the dicom data using matplotlib\n",
    "#%pip install ipywidgets # for scrollbar of multuple stacked slices\n",
    "\n",
    "\n",
    "#libraires\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import Optional, Dict, Any\n",
    "import logging\n",
    "import pydicom\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import to_rgba\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, IntSlider\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import vtk\n",
    "from vtk.util import numpy_support\n",
    "\n",
    "\n",
    "# functions\n",
    "def read_dicom(dicom_path):\n",
    "    \"\"\"\n",
    "    Reads a DICOM file from the given path.\n",
    "\n",
    "    Parameters:\n",
    "    - selected_SEGdicom_fullpath: The full path to the DICOM file.\n",
    "\n",
    "    Returns:\n",
    "    - The DICOM dataset if read successfully, otherwise None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dicom_data = pydicom.dcmread(dicom_path)\n",
    "        logging.info(f\"Dicom data loaded from: {dicom_path}.\")\n",
    "        return dicom_data\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in reading '{dicom_path}': {e}\")\n",
    "        return None\n",
    "\n",
    "# functions to use in future\n",
    "def get_nested_element(dataset, tags):\n",
    "    \"\"\"\n",
    "    Navigate through the DICOM dataset using a list of tags and return the final element.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (pydicom.dataset.Dataset): The DICOM dataset.\n",
    "        tags (list): A list of tuples representing the tags to navigate through.\n",
    "\n",
    "    Returns:\n",
    "        The final element in the DICOM dataset specified by the tags.\n",
    "    \"\"\"\n",
    "    current_element = dataset\n",
    "    for tag in tags:\n",
    "        tag = pydicom.tag.Tag(tag)\n",
    "        if tag in current_element:\n",
    "            current_element = current_element[tag]\n",
    "        else:\n",
    "            raise KeyError(f\"Tag {tag} not found in the dataset.\")\n",
    "        \n",
    "        # If the current element is a sequence, assume we want the first item\n",
    "        if isinstance(current_element, pydicom.sequence.Sequence):\n",
    "            if len(current_element) > 0:\n",
    "                current_element = current_element[0]\n",
    "            else:\n",
    "                raise ValueError(f\"Sequence at tag {tag} is empty.\")\n",
    "    \n",
    "    return current_element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find all patients directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_directory_folders(directory):\n",
    "    try:\n",
    "        # Get the absolute path of the directory\n",
    "        abs_directory = os.path.abspath(directory)\n",
    "        \n",
    "        # Check if the directory exists\n",
    "        if not os.path.isdir(abs_directory):\n",
    "            raise ValueError(f\"The specified path '{abs_directory}' is not a valid directory.\")\n",
    "        \n",
    "        # List all items in the directory\n",
    "        all_items = os.listdir(abs_directory)\n",
    "        \n",
    "        # Filter out non-directories and create full paths\n",
    "        folder_paths = [os.path.join(abs_directory, item) for item in all_items \n",
    "                        if os.path.isdir(os.path.join(abs_directory, item))]\n",
    "        \n",
    "        print(\"Number of folders:\" , len(folder_paths))\n",
    "        return folder_paths\n",
    "    \n",
    "    except PermissionError:\n",
    "        print(f\"Permission denied: Unable to access '{abs_directory}'.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Example usage (you can run this in a separate cell)\n",
    "patient_directories = list_directory_folders(project_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find all segmentations in a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assessors_path(patient_directory: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    This function takes the path to a patient directory and returns the path of the ASSESSORS directory if it exists.\n",
    "    \n",
    "    Parameters:\n",
    "    - patient_directory (str): The directory path of the patient.\n",
    "    \n",
    "    Returns:\n",
    "    - Optional[str]: The path to the ASSESSORS directory if it exists, otherwise None.\n",
    "    \"\"\"\n",
    "    \n",
    "    assessors_dir = os.path.join(patient_directory, 'ASSESSORS')\n",
    "    if not os.path.exists(assessors_dir):\n",
    "        logging.info(\"No segmentation exists\")\n",
    "        return None\n",
    "    logging.info(f\"'ASSESSORS' folder exists: {assessors_dir}\")\n",
    "    return assessors_dir\n",
    "\n",
    "\n",
    "def get_segmentations_from_assessors_path(assessors_path: str) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    This function searches the ASSESSORS folder and creates a dictionary of all segmentations.\n",
    "    \n",
    "    Parameters:\n",
    "    - assessors_path (str): The path to the ASSESSORS directory.\n",
    "    \n",
    "    Returns:\n",
    "    - Dict[str, Dict[str, Any]]: A dictionary with the name of the segmentor and datetime of segmentation as keys. \n",
    "      Each key maps to another dictionary containing 'created_by', 'created_time', 'dicom_name', and 'dicom_fullpath'.\n",
    "    \"\"\"\n",
    "    segmentation_paths = [d for d in os.listdir(assessors_path) if os.path.isdir(os.path.join(assessors_path, d))]\n",
    "    segmentations = {}\n",
    "\n",
    "    for seg in segmentation_paths:\n",
    "        seg_dir = os.path.join(assessors_path, seg, 'SEG')\n",
    "        if os.path.exists(seg_dir):\n",
    "            files = os.listdir(seg_dir)\n",
    "            \n",
    "            # Find XML files\n",
    "            xml_files = [f for f in files if f.endswith('.xml')]\n",
    "            \n",
    "            for xml_file in xml_files:\n",
    "                xml_path = os.path.join(seg_dir, xml_file)\n",
    "                try:\n",
    "                    # Parse the XML file\n",
    "                    tree = ET.parse(xml_path)\n",
    "                    root = tree.getroot()\n",
    "                    \n",
    "                    # Extract createdBy and createdTime\n",
    "                    entry = root.find('.//cat:entry', namespaces={'cat': 'http://nrg.wustl.edu/catalog'})\n",
    "                    if entry is not None:\n",
    "                        created_by = entry.get('createdBy')\n",
    "                        created_time = entry.get('createdTime')\n",
    "                        dicom_name = entry.get('name')\n",
    "                        dicom_fullpath = os.path.join(seg_dir, dicom_name)\n",
    "                        \n",
    "                        # Save to dictionary\n",
    "                        key = f\"{created_by}>>{created_time}\"\n",
    "                        segmentations[key] = {\n",
    "                            'created_by': created_by,\n",
    "                            'created_time': created_time,\n",
    "                            'dicom_name': dicom_name,\n",
    "                            'dicom_fullpath': dicom_fullpath\n",
    "                        }\n",
    "                        logging.info(segmentations[key])\n",
    "                except ET.ParseError as e:\n",
    "                    logging.error(f\"Error parsing XML file {xml_file}: {e}\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Unexpected error: {e}\")\n",
    "\n",
    "    return segmentations\n",
    "\n",
    "ass_path = assessors_path(patient_directory)                    \n",
    "all_segmentation_dic = get_segmentations_from_assessors_path(ass_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the desired segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_segmentation_from_valid(all_segmentation_dic: Dict[str, Dict[str, Any]], default_selected_segmentation: str = None) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    This function allows the user to select a segmentation from the dictionary of segmentations.\n",
    "    \n",
    "    Parameters:\n",
    "    - all_segmentation_dic (Dict[str, Dict[str, Any]]): Dictionary containing segmentations.\n",
    "    - default_selected_segmentation (str, optional): Default selected segmentation. Defaults to None.\n",
    "    \n",
    "    Returns:\n",
    "    - Optional[str]: The full path to the selected DICOM file, or None if not found.\n",
    "    \"\"\"\n",
    "    valid_options_to_select = list(all_segmentation_dic.keys())\n",
    "    len_options_to_select = len(valid_options_to_select)\n",
    "    \n",
    "    if len_options_to_select == 0:\n",
    "        logging.warning(\"No valid segmentations available.\")\n",
    "        return None\n",
    "    \n",
    "    if not default_selected_segmentation:\n",
    "        show_options_to_select = \"\\n\".join([f\"{i}: {valid_options_to_select[i]}\" for i in range(len_options_to_select)])\n",
    "        print(f\"Available segmentations:\\n{show_options_to_select}\")\n",
    "        selected_segmentation = input(\"Enter the index or name of your desired segmentation to visualize: \")\n",
    "    else:\n",
    "        selected_segmentation = default_selected_segmentation\n",
    "    \n",
    "    try:\n",
    "        selected_segmentation_index = int(selected_segmentation)\n",
    "        if selected_segmentation_index < 0 or selected_segmentation_index >= len_options_to_select:\n",
    "            raise IndexError\n",
    "        selected_segmentation_name = valid_options_to_select[selected_segmentation_index]\n",
    "    except (ValueError, IndexError):\n",
    "        selected_segmentation_name = selected_segmentation\n",
    "        \n",
    "    segmentation_info = all_segmentation_dic.get(selected_segmentation_name)\n",
    "    if segmentation_info:\n",
    "        logging.info(f\"Selected SEG: {selected_segmentation_name}\")\n",
    "        logging.info(f\"Path to selected SEG: {segmentation_info.get('dicom_fullpath')}\")\n",
    "        return segmentation_info.get('dicom_fullpath')\n",
    "    else:\n",
    "        logging.error(f\"Selected segmentation '{selected_segmentation_name}' not found in the dictionary.\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "selected_SEGdicom_fullpath = select_segmentation_from_valid(all_segmentation_dic,default_selected_segmentation=2)\n",
    "selected_SEGdicom_data = read_dicom(selected_SEGdicom_fullpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data of each segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_selected_segmentation_for_widgets(selected_segmentation_name, all_segmentation_dic):\n",
    "    segmentation_info = all_segmentation_dic.get(selected_segmentation_name)\n",
    "    if segmentation_info:\n",
    "        logging.info(f\"Selected SEG: {selected_segmentation_name}\")\n",
    "        logging.info(f\"Path to selected SEG: {segmentation_info.get('dicom_fullpath')}\")\n",
    "        return segmentation_info.get('dicom_fullpath')\n",
    "    else:\n",
    "        logging.error(f\"Selected segmentation '{selected_segmentation_name}' not found in the dictionary.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract segmentation file data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Series description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segmentation_description(segmentation_dicom) -> str:\n",
    "    \"\"\"\n",
    "    Extract a list of Referenced SOP Instance UIDs from a DICOM segmentation dataset.\n",
    "\n",
    "    Parameters:\n",
    "        segmentation_dicom (pydicom.dataset.Dataset): The DICOM dataset containing segmentation data.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of Referenced SOP Instance UIDs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        segmentation_description = get_nested_element(segmentation_dicom, [(0x0008, 0x103E)]).value\n",
    "        logging.info(f\"segmentation_description: {segmentation_description}\")\n",
    "        return segmentation_description\n",
    "    except (KeyError, ValueError) as e:\n",
    "        logging.error(f\"Error extracting segmentation_description: {e}\")\n",
    "\n",
    "segmentation_description = get_segmentation_description(selected_SEGdicom_data)\n",
    "segmentation_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference Series UID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_referenced_series_UID(segmentation_dicom) -> str:\n",
    "    \"\"\"\n",
    "    Extract a list of Referenced SOP Instance UIDs from a DICOM segmentation dataset.\n",
    "\n",
    "    Parameters:\n",
    "        segmentation_dicom (pydicom.dataset.Dataset): The DICOM dataset containing segmentation data.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of Referenced SOP Instance UIDs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        referenced_series_sequence = get_nested_element(segmentation_dicom, [(0x0008, 0x1115)])\n",
    "        for series_instance in referenced_series_sequence:\n",
    "            if 'SeriesInstanceUID' in series_instance:\n",
    "                Ref_series_UID = series_instance.SeriesInstanceUID\n",
    "                logging.info(f\"Referenced Series Instance UID: {Ref_series_UID}\")\n",
    "                return Ref_series_UID\n",
    "    except (KeyError, ValueError) as e:\n",
    "        logging.error(f\"Error extracting Referenced Series Instance UID: {e}\")\n",
    "\n",
    "Ref_series_UID = get_referenced_series_UID(selected_SEGdicom_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segmentation number-name dict map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_segment_number_to_label_map(segmentation_dicom) -> dict:\n",
    "    \"\"\"\n",
    "    Create a dictionary mapping Segment Number to Segment Label from a DICOM segmentation object.\n",
    "\n",
    "    Parameters:\n",
    "        segmentation_dicom (pydicom.dataset.Dataset): The DICOM dataset containing segmentation data.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping Segment Number (int) to Segment Label (str).\n",
    "    \"\"\"\n",
    "    segment_map = {}\n",
    "    try:\n",
    "        segment_sequence = get_nested_element(segmentation_dicom, [(0x0062, 0x0002)])\n",
    "        for item in segment_sequence:\n",
    "            if (0x0062, 0x0004) in item and (0x0062, 0x0005) in item:\n",
    "                segment_number = item[(0x0062, 0x0004)].value\n",
    "                segment_label = item[(0x0062, 0x0005)].value\n",
    "                segment_map[segment_number] = segment_label\n",
    "        logging.info(f\"Segment map: {segment_map}\")\n",
    "    except (KeyError, ValueError) as e:\n",
    "        logging.error(f\"Error creating segment map: {e}\")\n",
    "    return segment_map\n",
    "\n",
    "segment_map = create_segment_number_to_label_map(selected_SEGdicom_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### slice data: RefSOPUID + Segment number (and label) + pixel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segmentation_data_including_RefSOPUID_refSegNum_pixelData(segmentation_dicom, segment_map):\n",
    "    \"\"\"\n",
    "    Reads a segmentation DICOM file and creates a dictionary for each slice.\n",
    "\n",
    "    Parameters:\n",
    "    - segmentation_dicom: The DICOM dataset for the segmentation.\n",
    "    - segment_map: Dictionary to map segment numbers to segment labels.\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary with segment labels as keys and lists of dictionaries as values. Each dictionary contains 'pixel_data' and 'sop_instance_uid'.\n",
    "    \"\"\"\n",
    "    segmentation_data = {}\n",
    "\n",
    "    try:\n",
    "        # Get the number of frames\n",
    "        num_frames = segmentation_dicom.NumberOfFrames\n",
    "        logging.info(f\"Number of frames in the DICOM: {num_frames}\")\n",
    "\n",
    "        # Get pixel data\n",
    "        pixel_data = segmentation_dicom.pixel_array\n",
    "\n",
    "        # Get referenced instance UID and segmentation number for each frame\n",
    "        for frame_index in range(num_frames):\n",
    "            try:\n",
    "                frame = segmentation_dicom.PerFrameFunctionalGroupsSequence[frame_index]\n",
    "                referenced_sop_instance_uid = frame.DerivationImageSequence[0].SourceImageSequence[0].ReferencedSOPInstanceUID\n",
    "                image_position_patient=frame.PlanePositionSequence[0].ImagePositionPatient\n",
    "                segment_number = frame.SegmentIdentificationSequence[0].ReferencedSegmentNumber\n",
    "                \n",
    "                if segment_number in segment_map:\n",
    "                    segment_label = segment_map[segment_number]\n",
    "                    slice_info = {\n",
    "                        'pixel_data': pixel_data[frame_index],\n",
    "                        'sop_instance_uid': referenced_sop_instance_uid,\n",
    "                        'image_position_patient': image_position_patient\n",
    "                    }\n",
    "                    if segment_label not in segmentation_data:\n",
    "                        segmentation_data[segment_label] = []\n",
    "                    segmentation_data[segment_label].append(slice_info)\n",
    "                else:\n",
    "                    logging.warning(f\"Segment number {segment_number} not found in segment_map.\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing frame {frame_index}: {e}\")\n",
    "\n",
    "        # Log the count of slices for each segment label\n",
    "        for segment_label, slices in segmentation_data.items():\n",
    "            logging.info(f\"Segment '{segment_label}': {len(slices)} slices\")\n",
    "\n",
    "    except AttributeError as e:\n",
    "        logging.error(f\"Error reading DICOM attributes: {e}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error: {e}\")\n",
    "\n",
    "    return segmentation_data\n",
    "\n",
    "slice_info_list = get_segmentation_data_including_RefSOPUID_refSegNum_pixelData(selected_SEGdicom_data,segment_map)\n",
    "slice_info_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All segmentations in directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    # Remove invalid characters and replace with underscore\n",
    "    return re.sub(r'[<>:\"/\\\\|?*]', '_', filename)\n",
    "\n",
    "def find_all_segmentations_v2(project_directory, save_excel_path=None, save_pkl_path=None):\n",
    "    patient_directories = list_directory_folders(project_directory)\n",
    "    all_segmentation_df = pd.DataFrame(columns=['patient_directory', 'segmentation_name', 'segmentation_filepath'])\n",
    "    \n",
    "    for patient_directory in tqdm(patient_directories):\n",
    "        try:\n",
    "            ass_path = assessors_path(patient_directory)                    \n",
    "            all_segmentation_dic = get_segmentations_from_assessors_path(ass_path)\n",
    "            for selected_segmentation in all_segmentation_dic.keys():\n",
    "                \n",
    "                selected_SEGdicom_fullpath = select_segmentation_from_valid(all_segmentation_dic, default_selected_segmentation=selected_segmentation)\n",
    "                selected_SEGdicom_data = read_dicom(selected_SEGdicom_fullpath)\n",
    "\n",
    "                Ref_series_UID = get_referenced_series_UID(selected_SEGdicom_data)\n",
    "                segment_map = create_segment_number_to_label_map(selected_SEGdicom_data)\n",
    "                slice_info_list = get_segmentation_data_including_RefSOPUID_refSegNum_pixelData(selected_SEGdicom_data, segment_map)\n",
    "                segmentation_description = get_segmentation_description(selected_SEGdicom_data)\n",
    "                                \n",
    "                new_row = {\n",
    "                    'patient_directory': patient_directory,\n",
    "                    'segmentation_name': selected_segmentation,\n",
    "                    'segmentation_filepath': selected_SEGdicom_fullpath,\n",
    "                    'Ref_series_UID': Ref_series_UID,\n",
    "                    'segmentation_description':segmentation_description\n",
    "                }\n",
    "                \n",
    "                for segment_label, slices in slice_info_list.items():\n",
    "                    new_row[segment_label] = len(slices)\n",
    "                    \n",
    "                    if save_pkl_path:\n",
    "                        try:\n",
    "                            # Sanitize directory and file names\n",
    "                            patient_name = sanitize_filename(os.path.basename(patient_directory))\n",
    "                            safe_segmentation_name = sanitize_filename(selected_segmentation)\n",
    "                            safe_segment_label = sanitize_filename(segment_label)\n",
    "                            \n",
    "                            # Create directory structure for pickle files\n",
    "                            pkl_dir = os.path.join(save_pkl_path, patient_name, safe_segmentation_name)\n",
    "                            os.makedirs(pkl_dir, exist_ok=True)\n",
    "                            \n",
    "                            # Prepare the data to save in the pickle file\n",
    "                            pkl_data = {\n",
    "                                'Ref_series_UID': Ref_series_UID,\n",
    "                                'segmentation_description': segmentation_description,\n",
    "                                'case_number': patient_name,\n",
    "                                'safe_segmentation_name': safe_segmentation_name,\n",
    "                                'validation': \"finalized\",\n",
    "                                'slices': slices,\n",
    "                                \n",
    "                                \n",
    "                            }\n",
    "                            \n",
    "                            # Save pickle file\n",
    "                            pkl_filename = f\"{safe_segment_label}.pkl\"\n",
    "                            pkl_path = os.path.join(pkl_dir, pkl_filename)\n",
    "                            with open(pkl_path, 'wb') as f:\n",
    "                                pickle.dump(pkl_data, f)\n",
    "                        except Exception as pkl_error:\n",
    "                            print(f\"Error saving pickle for {patient_directory}, {selected_segmentation}, {segment_label}: {str(pkl_error)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            new_row = {\n",
    "                'patient_directory': patient_directory,\n",
    "                'segmentation_name': str(e),\n",
    "                'segmentation_filepath': str(e),\n",
    "                'Ref_series_UID': str(e),\n",
    "                'segmentation_description':str(e)\n",
    "            }\n",
    "            all_segmentation_df = pd.concat([all_segmentation_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    \n",
    "    if save_excel_path:\n",
    "        all_segmentation_df.to_excel(save_excel_path, index=False)\n",
    "    \n",
    "    return all_segmentation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    # Remove invalid characters and replace with underscore\n",
    "    return re.sub(r'[<>:\"/\\\\|?*]', '_', filename)\n",
    "\n",
    "def find_all_segmentations_v3(project_directory, save_excel_path=None, save_pkl_path=None):\n",
    "    patient_directories = list_directory_folders(project_directory)\n",
    "    all_segmentation_df = pd.DataFrame(columns=['patient_directory', 'segmentation_name', 'segmentation_filepath'])\n",
    "    \n",
    "    for patient_directory in tqdm(patient_directories):\n",
    "        try:\n",
    "            ass_path = assessors_path(patient_directory)                    \n",
    "            all_segmentation_dic = get_segmentations_from_assessors_path(ass_path)\n",
    "            for selected_segmentation in all_segmentation_dic.keys():\n",
    "                \n",
    "                selected_SEGdicom_fullpath = select_segmentation_from_valid(all_segmentation_dic, default_selected_segmentation=selected_segmentation)\n",
    "                selected_SEGdicom_data = read_dicom(selected_SEGdicom_fullpath)\n",
    "\n",
    "                Ref_series_UID = get_referenced_series_UID(selected_SEGdicom_data)\n",
    "                segment_map = create_segment_number_to_label_map(selected_SEGdicom_data)\n",
    "                slice_info_list = get_segmentation_data_including_RefSOPUID_refSegNum_pixelData(selected_SEGdicom_data, segment_map)\n",
    "                segmentation_description = get_segmentation_description(selected_SEGdicom_data)\n",
    "                                \n",
    "                new_row = {\n",
    "                    'patient_directory': patient_directory,\n",
    "                    'segmentation_name': selected_segmentation,\n",
    "                    'segmentation_filepath': selected_SEGdicom_fullpath,\n",
    "                    'Ref_series_UID': Ref_series_UID,\n",
    "                    'segmentation_description':segmentation_description\n",
    "                }\n",
    "                \n",
    "                for segment_label, slices in slice_info_list.items():\n",
    "                    new_row[segment_label] = len(slices)\n",
    "                    \n",
    "                    if save_pkl_path:\n",
    "                        try:\n",
    "                            # Sanitize directory and file names\n",
    "                            patient_name = sanitize_filename(os.path.basename(patient_directory))\n",
    "                            safe_segmentation_name = sanitize_filename(selected_segmentation)\n",
    "                            safe_segment_label = sanitize_filename(segment_label)\n",
    "                            safe_description = sanitize_filename(segmentation_description)\n",
    "                            \n",
    "                            # Create directory structure for pickle files\n",
    "                            pkl_dir = os.path.join(save_pkl_path, patient_name, f\"{safe_description}__{safe_segmentation_name}\")\n",
    "                            os.makedirs(pkl_dir, exist_ok=True)\n",
    "                            \n",
    "                            # Prepare the data to save in the pickle file\n",
    "                            pkl_data = {\n",
    "                                'Ref_series_UID': Ref_series_UID,\n",
    "                                'segmentation_description': segmentation_description,\n",
    "                                'case_number': patient_name,\n",
    "                                'safe_segmentation_name': safe_segmentation_name,\n",
    "                                'validation': \"finalized\",\n",
    "                                'slices': slices,\n",
    "                                \n",
    "                                \n",
    "                            }\n",
    "                            \n",
    "                            # Save pickle file\n",
    "                            pkl_filename = f\"{safe_segment_label}.pkl\"\n",
    "                            pkl_path = os.path.join(pkl_dir, pkl_filename)\n",
    "                            with open(pkl_path, 'wb') as f:\n",
    "                                pickle.dump(pkl_data, f)\n",
    "                        except Exception as pkl_error:\n",
    "                            print(f\"Error saving pickle for {patient_directory}, {selected_segmentation}, {segment_label}: {str(pkl_error)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            new_row = {\n",
    "                'patient_directory': patient_directory,\n",
    "                'segmentation_name': str(e),\n",
    "                'segmentation_filepath': str(e),\n",
    "                'Ref_series_UID': str(e),\n",
    "                'segmentation_description':str(e)\n",
    "            }\n",
    "            all_segmentation_df = pd.concat([all_segmentation_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    \n",
    "    if save_excel_path:\n",
    "        all_segmentation_df.to_excel(save_excel_path, index=False)\n",
    "    \n",
    "    return all_segmentation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run: Save pickels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_excel_path=r\"D:\\BATCH1_seg_cleaned_E2\\all_segmentations.xlsx\"\n",
    "# save_pkl_path=r\"D:\\BATCH1_seg_cleaned_E2\"\n",
    "# # all_segmentation_df=find_all_segmentations_v2(project_directory, save_excel_path=save_excel_path, save_pkl_path=save_pkl_path)\n",
    "# all_segmentation_df=find_all_segmentations_v3(project_directory, save_excel_path=save_excel_path, save_pkl_path=save_pkl_path)\n",
    "# all_segmentation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show pickle content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### show pickle content\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Replace 'your_file.pkl' with the path to your pickle file\n",
    "pickle_file_path = r\"C:\\Users\\LEGION\\Desktop\\BATCH1_seg_cleaned_E2_validated\\2105\\Salmanipour__AlirezaSoleimanpour__2024-01-24T09_20_47.856\\P.pkl\"\n",
    "\n",
    "# Open the pickle file and load the dictionary\n",
    "with open(pickle_file_path, 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "# Display the content of the dictionary\n",
    "\n",
    "data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Replace 'your_file.pkl' with the path to your pickle file\n",
    "pickle_file_path = r\"D:\\BATCH1_seg_cleaned_E2\\1000034\\FFF_elham_PAS1__elham_gpteam__2024-01-24T15_16_47.347\\P.pkl\"\n",
    "\n",
    "# Open the pickle file and load the dictionary\n",
    "with open(pickle_file_path, 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "# Display the content of the dictionary\n",
    "\n",
    "print(data.items())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle to final pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import logging\n",
    "\n",
    "def extract_series_data(patient_directory):\n",
    "    \"\"\"\n",
    "    Extracts series data from XML files within the SCANS directory of the given patient directory.\n",
    "\n",
    "    Parameters:\n",
    "    - patient_directory: Path to the patient's directory containing the SCANS folder.\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary containing series data.\n",
    "    \"\"\"\n",
    "    scans_dir = os.path.join(patient_directory, 'SCANS')\n",
    "    series_folders = [d for d in os.listdir(scans_dir) if os.path.isdir(os.path.join(scans_dir, d))]\n",
    "    \n",
    "    series_data = {}\n",
    "\n",
    "    for series in series_folders:\n",
    "        series_path = os.path.join(scans_dir, series, 'DICOM')\n",
    "        xml_files = [f for f in os.listdir(series_path) if f.endswith('.xml')]\n",
    "\n",
    "        if not xml_files:\n",
    "            logging.warning(f\"No XML files found in series directory: {series_path}\")\n",
    "            continue\n",
    "\n",
    "        # Assuming there's only one XML file per series directory\n",
    "        xml_file = xml_files[0]\n",
    "        xml_path = os.path.join(series_path, xml_file)\n",
    "\n",
    "        try:\n",
    "            tree = ET.parse(xml_path)\n",
    "            root = tree.getroot()\n",
    "\n",
    "            # Extract original_RefSeriesUID\n",
    "            ref_series_uid = root.attrib.get('UID', None)\n",
    "            if not ref_series_uid:\n",
    "                logging.warning(f\"No UID found in XML root for series {series}\")\n",
    "                continue\n",
    "\n",
    "            # Extract original_RefInstanceUID_dict\n",
    "            ref_instance_uid_dict = {}\n",
    "            for entry in root.findall('.//cat:entry', namespaces={'cat': 'http://nrg.wustl.edu/catalog'}):\n",
    "                uid = entry.attrib.get('UID')\n",
    "                uri = entry.attrib.get('URI')\n",
    "                if uid and uri:\n",
    "                    ref_instance_uid_dict[uid] = os.path.join(series_path, uri)\n",
    "\n",
    "            series_data[series] = {\n",
    "                'original_RefSeriesUID': ref_series_uid,\n",
    "                'original_RefInstanceUID_dict': ref_instance_uid_dict\n",
    "            }\n",
    "        except ET.ParseError as e:\n",
    "            logging.error(f\"Error parsing XML file {xml_file}: {e}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error processing series {series}: {e}\")\n",
    "            \n",
    "    logging.info(f\"Successfully read the XML file for {patient_directory}. Series and slice counts:\")\n",
    "    for key, value in series_data.items():\n",
    "        original_RefInstanceUID_dict = value.get('original_RefInstanceUID_dict',{})\n",
    "        logging.info(f\"    {key}: {len(original_RefInstanceUID_dict)}\")    \n",
    "        \n",
    "    return series_data\n",
    "\n",
    "def get_path_to_matched_original_series(original_series_data,Ref_series_UID, patient_directory):\n",
    "    for original_folder, original_info_dic in original_series_data.items():\n",
    "        if original_info_dic['original_RefSeriesUID']==Ref_series_UID:\n",
    "            logging.info(f\"Successfully find a match for series UIDs in folder: '{original_folder}'\")\n",
    "            \n",
    "            matched_series_folder = os.path.join(patient_directory,\"SCANS\", original_folder,\"DICOM\")\n",
    " \n",
    "    if matched_series_folder:\n",
    "        logging.info(\" The matched series of original image created successfully.\")\n",
    "        return  matched_series_folder\n",
    "    else:\n",
    "        logging.warning(\"No matched series UID was \")\n",
    "        \n",
    "        \n",
    "def read_origignal_dicom_pixel_data(directory_path):\n",
    "    \"\"\"\n",
    "    Reads pixel data and referenced SOP Instance UID from all DICOM files in the given directory.\n",
    "\n",
    "    Parameters:\n",
    "    - directory_path: Path to the directory containing DICOM files.\n",
    "\n",
    "    Returns:\n",
    "    - List of dictionaries with 'pixel_data', 'image_position_patient', and 'instance_number'.\n",
    "    \"\"\"\n",
    "    dicom_files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith('.dcm')]\n",
    "    dicom_data_list = []\n",
    "    \n",
    "    for dicom_file in dicom_files:\n",
    "        try:\n",
    "            dicom_data = pydicom.dcmread(dicom_file)\n",
    "            sop_instance_uid = dicom_data.SOPInstanceUID\n",
    "            image_position_patient = dicom_data.ImagePositionPatient\n",
    "            pixel_data = dicom_data.pixel_array\n",
    "            instance_number = dicom_data.InstanceNumber if hasattr(dicom_data, 'InstanceNumber') else 0\n",
    "            \n",
    "            dicom_data_list.append({\n",
    "                'pixel_data': pixel_data,\n",
    "                'sop_instance_uid': sop_instance_uid,\n",
    "                'image_position_patient': image_position_patient,\n",
    "                'instance_number': instance_number\n",
    "            })\n",
    "            \n",
    "\n",
    "            Pixel_Spacing = dicom_data.PixelSpacing\n",
    "\n",
    "            Slice_Thickness= dicom_data.SliceThickness\n",
    "\n",
    "            Series_Description = dicom_data.SeriesDescription\n",
    "            \n",
    "            dicom_meta_data = {\n",
    "                \"Pixel_Spacing\": Pixel_Spacing,\n",
    "                \"Slice_Thickness\": Slice_Thickness,\n",
    "                \"Series_Description\": Series_Description\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error reading DICOM file '{dicom_file}': {e}\")\n",
    "            dicom_meta_data=None\n",
    "\n",
    "    \n",
    "    # Sort by InstanceNumber\n",
    "    dicom_data_list.sort(key=lambda x: x['instance_number'])\n",
    "    \n",
    "    return dicom_data_list, dicom_meta_data\n",
    "\n",
    "def merge_dictionaries_bySOP(original_dic, segment_dic):\n",
    "    \"\"\"\n",
    "    Merges two dictionaries based on the SOP Instance UID.\n",
    "\n",
    "    Parameters:\n",
    "    - original_dic: List of dictionaries with 'pixel_data', 'sop_instance_uid', and other keys.\n",
    "    - segment_dic: List of dictionaries with 'pixel_data', 'sop_instance_uid', and other keys.\n",
    "\n",
    "    Returns:\n",
    "    - Merged list of dictionaries.\n",
    "    \"\"\"\n",
    "    merged_data = []\n",
    "    segment_lookup = {item['sop_instance_uid']: item['pixel_data'] for item in segment_dic}\n",
    "    \n",
    "    for original_item in original_dic:\n",
    "        sop_instance_uid = original_item['sop_instance_uid']\n",
    "        segmentation_pixel = segment_lookup.get(sop_instance_uid, None)\n",
    "        \n",
    "        merged_data.append({\n",
    "            'sop_instance_uid': sop_instance_uid,\n",
    "            'image_position_patient': original_item['image_position_patient'],\n",
    "            'original_pixel': original_item['pixel_data'],\n",
    "            'segmentation_pixel': segmentation_pixel,\n",
    "            'instance_number': original_item.get('instance_number')\n",
    "        })\n",
    "    \n",
    "    return merged_data\n",
    "\n",
    "def merge_dictionaries_byPatientPosition(original_dic, segment_dic):\n",
    "    \"\"\"\n",
    "    Merges two dictionaries based on the SOP Instance UID.\n",
    "\n",
    "    Parameters:\n",
    "    - original_dic: List of dictionaries with 'pixel_data', 'sop_instance_uid', and other keys.\n",
    "    - segment_dic: List of dictionaries with 'pixel_data', 'sop_instance_uid', and other keys.\n",
    "\n",
    "    Returns:\n",
    "    - Merged list of dictionaries.\n",
    "    \"\"\"\n",
    "    merged_data = []\n",
    "    segment_lookup = {str(item['image_position_patient']): item['pixel_data'] for item in segment_dic}\n",
    "    \n",
    "    for original_item in original_dic:\n",
    "        image_position_patient = str(original_item['image_position_patient'])\n",
    "        segmentation_pixel = segment_lookup.get(image_position_patient, None)\n",
    "        \n",
    "        merged_data.append({\n",
    "            'sop_instance_uid': original_item['sop_instance_uid'], \n",
    "            'image_position_patient': image_position_patient,\n",
    "            'original_pixel': original_item['pixel_data'],\n",
    "            'segmentation_pixel': segmentation_pixel,\n",
    "            'instance_number': original_item.get('instance_number')\n",
    "        })\n",
    "    \n",
    "    return merged_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "\n",
    "# Function to process the segmentation and merge with original CT\n",
    "def process_segmentation_pickle(pickle_path, project_directory, save_pkl_directory):\n",
    "    \"\"\"\n",
    "    Processes the segmentation pickle file and merges the segmentation with the original CT series.\n",
    "\n",
    "    Parameters:\n",
    "    - pickle_path: Path to the segmentation pickle file.\n",
    "    - project_directory: Directory containing patient folders.\n",
    "    - save_directory: Directory to save the updated pickle with merged data.\n",
    "\n",
    "    Returns:\n",
    "    - Path to the updated pickle file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the segmentation pickle file\n",
    "    with open(pickle_path, 'rb') as f:\n",
    "        pkl_data = pickle.load(f)\n",
    "\n",
    "    # Extract case_number and Ref_series_UID from the pickle data\n",
    "    case_number = pkl_data.get('case_number')\n",
    "    Ref_series_UID = pkl_data.get('Ref_series_UID')\n",
    "\n",
    "    # Find the patient folder using case_number\n",
    "    patient_directory = os.path.join(project_directory, case_number)\n",
    "    if not os.path.exists(patient_directory):\n",
    "        logging.error(f\"Patient directory {patient_directory} not found.\")\n",
    "        return None\n",
    "\n",
    "    # Use extract_series_data to get series information\n",
    "    original_series_data = extract_series_data(patient_directory)\n",
    "\n",
    "    # Find the matched series for Ref_series_UID\n",
    "    matched_series_folder = get_path_to_matched_original_series(original_series_data, Ref_series_UID, patient_directory)\n",
    "    if not matched_series_folder:\n",
    "        logging.error(f\"No matching series found for UID: {Ref_series_UID}\")\n",
    "        return None\n",
    "\n",
    "    \n",
    "    dicom_data_list, dicom_meta_data = read_origignal_dicom_pixel_data(matched_series_folder)\n",
    "    \n",
    "    # Merge the segmentation slices with the original CT data using merge_ct_and_segmentation function\n",
    "    #merged_data = merge_dictionaries_bySOP(dicom_data_list, pkl_data['slices'])\n",
    "    merged_data = merge_dictionaries_byPatientPosition(dicom_data_list, pkl_data['slices'])\n",
    "    \n",
    "    print(f\"number of slices in merged_data: {len(merged_data)}\")\n",
    "    merged_slices_count=0\n",
    "    for dic in merged_data:\n",
    "        if dic['segmentation_pixel'] is not None:\n",
    "            merged_slices_count+=1\n",
    "    print(f\"number of segmentation (merged) slices merged_data {merged_slices_count}\")\n",
    "    merged_slices_count\n",
    "\n",
    "    \n",
    "    dicom_meta_data['case_number']=case_number\n",
    "    dicom_meta_data['Ref_series_UID']=Ref_series_UID\n",
    "    dicom_meta_data['validation_level']='3_finalized'\n",
    "    \n",
    "    processed_output_dic = {\n",
    "        \"dicom_meta_data\": dicom_meta_data,\n",
    "        \"merged_slices\": merged_data\n",
    "    }\n",
    "\n",
    "    if save_pkl_directory:\n",
    "        # Save the merged data in a new pickle file\n",
    "        case_folder = os.path.basename(os.path.dirname(os.path.dirname(pickle_path)))\n",
    "        subfolder = os.path.basename(os.path.dirname(pickle_path))\n",
    "        filename = os.path.basename(pickle_path)\n",
    "        \n",
    "        updated_pickle_path = os.path.join(save_pkl_directory, case_folder, subfolder, filename)\n",
    "        \n",
    "        # Ensure the directory exists\n",
    "        os.makedirs(os.path.dirname(updated_pickle_path), exist_ok=True)\n",
    "\n",
    "        with open(updated_pickle_path, 'wb') as f:\n",
    "            pickle.dump(processed_output_dic, f)\n",
    "        logging.info(f\"Updated pickle file saved at: {updated_pickle_path}\")\n",
    "\n",
    "    return processed_output_dic\n",
    "\n",
    "# Example usage\n",
    "pickle_file_path = r\"D:\\BATCH1_seg_cleaned_E2\\1000034\\FFF_elham_PAS1__elham_gpteam__2024-01-24T15_16_47.347\\P.pkl\"\n",
    "project_dir = r\"D:\\PanCanAID-Batch1-202408\"\n",
    "save_pkl_directory = r'D:\\BATCH1_seg_merged'\n",
    "\n",
    "# desired save location r\"D:\\BATCH1_seg_merged\\1000034\\FFF_elham_PAS1__elham_gpteam__2024-01-24T15_16_47.347\\P.pkl\"\n",
    "\n",
    "processed_output_dic = process_segmentation_pickle(pickle_file_path, project_dir, save_pkl_directory)\n",
    "processed_output_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def process_multiple_pickles(pickle_names, source_directory, project_directory, save_pkl_directory):\n",
    "    \"\"\"\n",
    "    Process multiple pickle files based on provided names.\n",
    "\n",
    "    Parameters:\n",
    "    - pickle_names: List of strings, each representing a pickle file name to process.\n",
    "    - source_directory: Directory to search for pickle files.\n",
    "    - project_directory: Directory containing patient folders (passed to process_segmentation_pickle).\n",
    "    - save_pkl_directory: Directory to save processed pickles (passed to process_segmentation_pickle).\n",
    "\n",
    "    Returns:\n",
    "    - A list of processed output dictionaries.\n",
    "    \"\"\"\n",
    "    processed_outputs = []\n",
    "\n",
    "    # Recursively search for all .pkl files in the source_directory\n",
    "    all_pickles = glob.glob(os.path.join(source_directory, '**', '*.pkl'), recursive=True)\n",
    "\n",
    "    for pickle_name in tqdm(pickle_names):\n",
    "        # Find matching pickle files\n",
    "        matching_pickles = [p for p in all_pickles if os.path.basename(p) == pickle_name]\n",
    "\n",
    "        if not matching_pickles:\n",
    "            logging.warning(f\"No pickle file found with name: {pickle_name}\")\n",
    "            continue\n",
    "\n",
    "        for pickle_path in tqdm(matching_pickles):\n",
    "            logging.info(f\"Processing pickle file: {pickle_path}\")\n",
    "            \n",
    "            try:\n",
    "                processed_output = process_segmentation_pickle(pickle_path, project_directory, save_pkl_directory)\n",
    "                if processed_output:\n",
    "                    processed_outputs.append(processed_output)\n",
    "                    logging.info(f\"Successfully processed: {pickle_path}\")\n",
    "                else:\n",
    "                    logging.warning(f\"Failed to process: {pickle_path}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing {pickle_path}: {str(e)}\")\n",
    "\n",
    "    return processed_outputs\n",
    "\n",
    "# Example usage:\n",
    "pickle_names = [\"P.pkl\", \"p.pkl\", 'Pancreas.pkl', 'pancreas.pkl', 'm.pkl','M.pkl', 'mass.pkl', 'Mass.pkl', 'mpd.pkl' ,'MPD.pkl' ]\n",
    "source_directory = r\"D:\\Batch1_20240830\\BATCH1_seg_clean_semiValid\"\n",
    "project_directory = r\"D:\\Batch1_20240830\\PanCanAID-Batch1-202408\"\n",
    "save_pkl_directory = r\"D:\\Batch1_20240830\\BATCH1_seg_merged_Ps\"\n",
    "\n",
    "results = process_multiple_pickles(pickle_names, source_directory, project_directory, save_pkl_directory)\n",
    "print(f\"Processed {len(results)} pickle files successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play with a pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show pickle content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Replace 'your_file.pkl' with the path to your pickle file\n",
    "pickle_file_path = r\"C:\\Users\\LEGION\\Desktop\\BATCH1_seg_clean_semiValid\\2128\\AAA_validated__admin__2024-09-10T07_56_44.721\\G.pkl\"\n",
    "# Open the pickle file and load the dictionary\n",
    "with open(pickle_file_path, 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "# Display the content of the dictionary\n",
    "\n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modify pickle validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog, messagebox\n",
    "\n",
    "def modify_validation_in_pickle(pickle_path):\n",
    "    \"\"\"\n",
    "    Load a pickle file, display a popup to modify the 'validation' item,\n",
    "    and save the modified pickle file.\n",
    "\n",
    "    Parameters:\n",
    "    - pickle_path: Path to the pickle file to modify.\n",
    "\n",
    "    Returns:\n",
    "    - The modified dictionary if successful, None otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the pickle file\n",
    "        with open(pickle_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        # Check if 'validation' exists in the dictionary\n",
    "        if 'validation' not in data:\n",
    "            messagebox.showerror(\"Error\", \"'validation' key not found in the pickle file.\")\n",
    "            return None\n",
    "\n",
    "        # Create the main window\n",
    "        root = tk.Tk()\n",
    "        root.withdraw()  # Hide the main window\n",
    "\n",
    "        # Show a dialog to get the new validation value\n",
    "        current_validation = data['validation']\n",
    "        new_validation = simpledialog.askstring(\"Modify Validation\", \n",
    "                                                f\"Current validation: {current_validation}\\n\"\n",
    "                                                \"Enter new validation value:\",\n",
    "                                                initialvalue=current_validation)\n",
    "\n",
    "        # If user cancels, return None\n",
    "        if new_validation is None:\n",
    "            return None\n",
    "\n",
    "        # Update the validation in the dictionary\n",
    "        data['validation'] = new_validation\n",
    "\n",
    "        # Save the modified dictionary back to the pickle file\n",
    "        with open(pickle_path, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "\n",
    "        messagebox.showinfo(\"Success\", f\"Validation updated to: {new_validation}\")\n",
    "        return data\n",
    "\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "# pickle_path = r\"path\\to\\your\\pickle\\file.pkl\"\n",
    "# modified_data = modify_validation_in_pickle(pickle_path)\n",
    "\n",
    "# if modified_data:\n",
    "#     print(\"Pickle file successfully modified.\")\n",
    "#     print(f\"New validation value: {modified_data['validation']}\")\n",
    "# else:\n",
    "#     print(\"Failed to modify pickle file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_paths=[\n",
    "    r\"C:\\Users\\LEGION\\Desktop\\BATCH1_seg_cleaned_E2_validated\\2166\\AHMAD SHOJA__AhmadShoja__2024-01-05T18_01_49.533\\M.pkl\",\n",
    "    r\"C:\\Users\\LEGION\\Desktop\\BATCH1_seg_cleaned_E2_validated\\2166\\AHMAD SHOJA__AhmadShoja__2024-01-05T18_01_49.533\\MPD.pkl\",\n",
    "    r\"C:\\Users\\LEGION\\Desktop\\BATCH1_seg_cleaned_E2_validated\\2166\\AHMAD SHOJA__AhmadShoja__2024-01-05T18_01_49.533\\P.pkl\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pickle_path in pickle_paths:\n",
    "#     modified_data = modify_validation_in_pickle(pickle_path)\n",
    "\n",
    "#     if modified_data:\n",
    "#         print(\"Pickle file successfully modified.\")\n",
    "#         print(f\"New validation value: {modified_data['validation']}\")\n",
    "#     else:\n",
    "#         print(\"Failed to modify pickle file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ipywidgets import interact, interactive, fixed, widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def visualize_segmentation_pickle(pickle_file_path):\n",
    "    # Load the pickle file\n",
    "    with open(pickle_file_path, 'rb') as f:\n",
    "        all_segmentation_data = pickle.load(f)\n",
    "        \n",
    "    segmentation_data = all_segmentation_data['slices']\n",
    "    # Check if the data is a list of dictionaries\n",
    "    if not isinstance(segmentation_data, list) or not all(isinstance(item, dict) for item in segmentation_data):\n",
    "        raise ValueError(\"The pickle file should contain a list of dictionaries.\")\n",
    "    \n",
    "    # Extract pixel data and positions\n",
    "    pixel_data_list = [item['pixel_data'] for item in segmentation_data]\n",
    "    positions = [item['image_position_patient'] for item in segmentation_data]\n",
    "    \n",
    "    # Create a 3D volume\n",
    "    volume = np.stack(pixel_data_list, axis=0)\n",
    "    \n",
    "    # Function to view a slice\n",
    "    def view_slice(axis, slice_num):\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        if axis == 'Axial':\n",
    "            plt.imshow(volume[slice_num, :, :], cmap='gray')\n",
    "            plt.title(f'Axial Slice {slice_num}')\n",
    "        elif axis == 'Coronal':\n",
    "            plt.imshow(volume[:, slice_num, :], cmap='gray')\n",
    "            plt.title(f'Coronal Slice {slice_num}')\n",
    "        else:  # Sagittal\n",
    "            plt.imshow(volume[:, :, slice_num], cmap='gray')\n",
    "            plt.title(f'Sagittal Slice {slice_num}')\n",
    "        plt.colorbar(label='Pixel Value')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "    # Create interactive widget\n",
    "    axis_widget = widgets.Dropdown(options=['Axial', 'Coronal', 'Sagittal'], value='Axial', description='View:')\n",
    "    slice_widget = widgets.IntSlider(min=0, max=volume.shape[0]-1, step=1, value=volume.shape[0]//2, description='Slice:')\n",
    "    \n",
    "    # Update slice widget based on selected axis\n",
    "    def update_slice_widget(*args):\n",
    "        axis = axis_widget.value\n",
    "        if axis == 'Axial':\n",
    "            slice_widget.max = volume.shape[0] - 1\n",
    "        elif axis == 'Coronal':\n",
    "            slice_widget.max = volume.shape[1] - 1\n",
    "        else:  # Sagittal\n",
    "            slice_widget.max = volume.shape[2] - 1\n",
    "        slice_widget.value = slice_widget.max // 2\n",
    "    \n",
    "    axis_widget.observe(update_slice_widget, 'value')\n",
    "    \n",
    "    # Display interactive widget\n",
    "    interactive_plot = interactive(view_slice, axis=axis_widget, slice_num=slice_widget)\n",
    "    display(interactive_plot)\n",
    "    \n",
    "    # Print some information about the segmentation\n",
    "    print(f\"Number of slices: {len(segmentation_data)}\")\n",
    "    print(f\"Dimensions of volume: {volume.shape}\")\n",
    "    print(f\"Range of pixel values: {np.min(volume)} to {np.max(volume)}\")\n",
    "\n",
    "# Example usage:\n",
    "pickle_path=r\"C:\\Users\\LEGION\\Desktop\\BATCH1_seg_cleaned_E2_validated\\2282\\ahmad shoja__AhmadShoja__2024-01-05T16_02_14.157\\p.pkl\"\n",
    "visualize_segmentation_pickle(pickle_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import vtk\n",
    "from vtk.util import numpy_support\n",
    "\n",
    "def create_3d_visualization(pickle_file_path, smoothing=True, morphology=True, advanced_smoothing=True):\n",
    "    # Load the pickle file\n",
    "    with open(pickle_file_path, 'rb') as f:\n",
    "        segmentation_data = pickle.load(f)\n",
    "    \n",
    "    # Extract pixel data and positions\n",
    "    pixel_data_list = [item['pixel_data'] for item in segmentation_data]\n",
    "    positions = [item['image_position_patient'] for item in segmentation_data]\n",
    "    \n",
    "    # Calculate slice spacing\n",
    "    slice_positions = [pos[2] for pos in positions]  # Assuming Z is the slice direction\n",
    "    slice_spacing = np.mean(np.diff(sorted(slice_positions)))\n",
    "    \n",
    "    # Get dimensions of a single slice\n",
    "    slice_shape = pixel_data_list[0].shape\n",
    "    \n",
    "    # Create a 3D volume with correct spacing\n",
    "    volume = np.stack(pixel_data_list, axis=0)\n",
    "    \n",
    "    # Create VTK image data\n",
    "    vtk_image = vtk.vtkImageData()\n",
    "    vtk_image.SetDimensions(slice_shape[1], slice_shape[0], len(pixel_data_list))\n",
    "    vtk_image.SetSpacing(1.0, 1.0, slice_spacing)  # Assuming 1mm pixel spacing in X and Y\n",
    "    vtk_image.SetOrigin(positions[0][0], positions[0][1], positions[0][2])  # Set origin to first slice position\n",
    "    \n",
    "    # Flatten the numpy array and convert to VTK array\n",
    "    vtk_array = numpy_support.numpy_to_vtk(volume.ravel(), deep=True, array_type=vtk.VTK_UNSIGNED_CHAR)\n",
    "    vtk_image.GetPointData().SetScalars(vtk_array)\n",
    "    \n",
    "    if morphology:\n",
    "        # Apply morphological operations\n",
    "        morph_filter = vtk.vtkImageOpenClose3D()\n",
    "        morph_filter.SetInputData(vtk_image)\n",
    "        morph_filter.SetKernelSize(3, 3, 3)\n",
    "        morph_filter.SetOpenValue(0)\n",
    "        morph_filter.SetCloseValue(1)\n",
    "        morph_filter.Update()\n",
    "        vtk_image = morph_filter.GetOutput()\n",
    "    \n",
    "    # Create a surface from the image data\n",
    "    contour = vtk.vtkMarchingCubes()\n",
    "    contour.SetInputData(vtk_image)\n",
    "    contour.SetValue(0, 1)  # Assuming binary segmentation, adjust if necessary\n",
    "    contour.Update()\n",
    "    \n",
    "    if smoothing:\n",
    "        # Apply initial smoothing\n",
    "        smoother = vtk.vtkSmoothPolyDataFilter()\n",
    "        smoother.SetInputConnection(contour.GetOutputPort())\n",
    "        smoother.SetNumberOfIterations(15)\n",
    "        smoother.SetRelaxationFactor(0.1)\n",
    "        smoother.FeatureEdgeSmoothingOff()\n",
    "        smoother.BoundarySmoothingOn()\n",
    "        smoother.Update()\n",
    "    else:\n",
    "        smoother = contour\n",
    "    \n",
    "    if advanced_smoothing:\n",
    "        # Apply windowed sinc smoothing\n",
    "        sinc_smoother = vtk.vtkWindowedSincPolyDataFilter()\n",
    "        sinc_smoother.SetInputConnection(smoother.GetOutputPort())\n",
    "        sinc_smoother.SetNumberOfIterations(20)\n",
    "        sinc_smoother.BoundarySmoothingOff()\n",
    "        sinc_smoother.FeatureEdgeSmoothingOff()\n",
    "        sinc_smoother.SetFeatureAngle(120.0)\n",
    "        sinc_smoother.SetPassBand(0.001)\n",
    "        sinc_smoother.NonManifoldSmoothingOn()\n",
    "        sinc_smoother.NormalizeCoordinatesOn()\n",
    "        sinc_smoother.Update()\n",
    "        \n",
    "        # Subdivide the surface\n",
    "        subdivisionFilter = vtk.vtkLinearSubdivisionFilter()\n",
    "        subdivisionFilter.SetInputConnection(sinc_smoother.GetOutputPort())\n",
    "        subdivisionFilter.SetNumberOfSubdivisions(1)\n",
    "        subdivisionFilter.Update()\n",
    "        \n",
    "        # Decimate (simplify) the mesh\n",
    "        decimate = vtk.vtkDecimatePro()\n",
    "        decimate.SetInputConnection(subdivisionFilter.GetOutputPort())\n",
    "        decimate.SetTargetReduction(0.1)  # Reduce number of triangles by 10%\n",
    "        decimate.PreserveTopologyOn()\n",
    "        decimate.Update()\n",
    "        \n",
    "        # Final smoothing pass\n",
    "        final_smoother = vtk.vtkSmoothPolyDataFilter()\n",
    "        final_smoother.SetInputConnection(decimate.GetOutputPort())\n",
    "        final_smoother.SetNumberOfIterations(10)\n",
    "        final_smoother.SetRelaxationFactor(0.1)\n",
    "        final_smoother.FeatureEdgeSmoothingOff()\n",
    "        final_smoother.BoundarySmoothingOn()\n",
    "        final_smoother.Update()\n",
    "        \n",
    "        # Create a mapper\n",
    "        mapper = vtk.vtkPolyDataMapper()\n",
    "        mapper.SetInputConnection(final_smoother.GetOutputPort())\n",
    "    else:\n",
    "        # Create a mapper without advanced smoothing\n",
    "        mapper = vtk.vtkPolyDataMapper()\n",
    "        mapper.SetInputConnection(smoother.GetOutputPort())\n",
    "    \n",
    "    # Create an actor\n",
    "    actor = vtk.vtkActor()\n",
    "    actor.SetMapper(mapper)\n",
    "    \n",
    "    # Create a renderer and render window\n",
    "    renderer = vtk.vtkRenderer()\n",
    "    render_window = vtk.vtkRenderWindow()\n",
    "    render_window.AddRenderer(renderer)\n",
    "    \n",
    "    # Create a render window interactor\n",
    "    interactor = vtk.vtkRenderWindowInteractor()\n",
    "    interactor.SetRenderWindow(render_window)\n",
    "    \n",
    "    # Add the actor to the scene\n",
    "    renderer.AddActor(actor)\n",
    "    renderer.SetBackground(0.1, 0.2, 0.4)  # Set background color\n",
    "    \n",
    "    # Reset camera and render\n",
    "    renderer.ResetCamera()\n",
    "    render_window.Render()\n",
    "    \n",
    "    # Start the interactor\n",
    "    interactor.Start()\n",
    "\n",
    "# Example usage:\n",
    "pickle_path=r\"D:\\BATCH1_seg_cleaned\\2248\\elham_gpteam__2024-07-16T15_30_33.257\\P.pkl\"\n",
    "create_3d_visualization(pickle_path, smoothing=True, morphology=True, advanced_smoothing=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DICOM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
